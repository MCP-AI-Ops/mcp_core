"""
ì›ë³¸ demoMCPproject.ipynb ê¸°ë°˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- CompleteMCPPredictor í´ë˜ìŠ¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ë¡œì»¬ í™˜ê²½ ê²½ë¡œë¡œ ìˆ˜ì •
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, regularizers, callbacks
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ìš©
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸš€ MCP LSTM ëª¨ë¸ í•™ìŠµ (ì›ë³¸ ë…¸íŠ¸ë¶ ê¸°ë°˜)")
print("=" * 80)

# GPU ìµœì í™” ì„¤ì •
print("\nğŸ“‹ í™˜ê²½ ì •ë³´:")
print(f"  TensorFlow ë²„ì „: {tf.__version__}")
print(f"  GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}")

# ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
def set_random_seeds(seed=42):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_random_seeds(42)

# ===================================================================
# CompleteMCPPredictor í´ë˜ìŠ¤ (ì›ë³¸ ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
# ===================================================================

class CompleteMCPPredictor:
    """
    Google Cluster ë°ì´í„°ì˜ ê·¹ê°’ê³¼ long-tail ë¶„í¬ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì„¤ê³„
    """

    def __init__(self,
                 sequence_length=24,
                 target_col='total_events',
                 test_size=0.2,
                 val_size=0.1,
                 use_log_transform=True,
                 handle_outliers=True):

        self.seq_len = sequence_length
        self.target_col = target_col
        self.test_size = test_size
        self.val_size = val_size
        self.use_log_transform = use_log_transform
        self.handle_outliers = handle_outliers

        # ìŠ¤ì¼€ì¼ëŸ¬ë“¤ (RobustScaler - ê·¹ê°’ì— ê°•ê±´)
        self.feature_scaler = RobustScaler()
        self.target_scaler = RobustScaler()

        # ëª¨ë¸ê³¼ ë°ì´í„°
        self.model = None
        self.history = None
        self.feature_names = None

        # ë°ì´í„° ì €ì¥
        self.X_train = self.X_val = self.X_test = None
        self.y_train = self.y_val = self.y_test = None
        self.df = None

    def load_and_prepare_data(self, csv_path='data/lstm_ready_cluster_data.csv'):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.df.shape}")

        # ê¸°ë³¸ ì •ë¦¬
        if 'hour_offset' in self.df.columns:
            self.df = self.df.sort_values('hour_offset').reset_index(drop=True)

        # íƒ€ê²Ÿ ë³€ìˆ˜ ì „ì²˜ë¦¬
        self._preprocess_target()

        # íŠ¹ì„± ì„ íƒ
        self._select_features()

        # ì´ìƒì¹˜ ì²˜ë¦¬
        if self.handle_outliers:
            self._handle_outliers()

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        self._handle_missing_values()

        print(f"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ: {self.df.shape}")
        print(f"âœ“ íƒ€ê²Ÿ í†µê³„ - í‰ê· : {self.df[self.target_col].mean():.2f}, "
              f"ë¶„ì‚°: {self.df[self.target_col].var():.2f}")

        return self.df

    def _preprocess_target(self):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ì „ì²˜ë¦¬"""
        if self.target_col not in self.df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{self.target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        target = self.df[self.target_col].copy()

        # ìŒìˆ˜ ê°’ ì²˜ë¦¬
        if (target < 0).any():
            print(f"âš ï¸  ìŒìˆ˜ ê°’ ë°œê²¬: {(target < 0).sum()}ê°œ")
            target = target.clip(lower=0)

        # ë¡œê·¸ ë³€í™˜ (ê·¹ë‹¨ê°’ ì²˜ë¦¬)
        if self.use_log_transform:
            print(f"âœ“ ë¡œê·¸ ë³€í™˜ ì ìš© (ê·¹ë‹¨ê°’ ì²˜ë¦¬)")
            target = np.log1p(target)

        self.df[self.target_col] = target

    def _select_features(self):
        """í”¼ì²˜ ì„ íƒ"""
        # ì œì™¸í•  ì»¬ëŸ¼
        exclude_cols = [self.target_col, 'hour_offset']
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        self.feature_names = [
            col for col in numeric_cols 
            if col not in exclude_cols and 'Unnamed' not in col
        ]
        
        print(f"âœ“ ì„ íƒëœ í”¼ì²˜: {len(self.feature_names)}ê°œ")

    def _handle_outliers(self):
        """ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ë°©ì‹)"""
        for col in self.feature_names:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            
            self.df[col] = self.df[col].clip(lower=lower_bound, upper=upper_bound)

    def _handle_missing_values(self):
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
        if self.df[self.feature_names + [self.target_col]].isnull().sum().sum() > 0:
            print("âš ï¸  ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
            self.df[self.feature_names + [self.target_col]] = \
                self.df[self.feature_names + [self.target_col]].fillna(method='ffill').fillna(method='bfill')

    def create_sequences(self):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸ”„ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (length={self.seq_len})")
        
        # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        features = self.df[self.feature_names].values
        features_scaled = self.feature_scaler.fit_transform(features)
        
        # íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ë§
        target = self.df[self.target_col].values
        target_scaled = self.target_scaler.fit_transform(target.reshape(-1, 1)).flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []
        for i in range(len(self.df) - self.seq_len):
            X.append(features_scaled[i:i+self.seq_len])
            y.append(target_scaled[i+self.seq_len])
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"âœ“ X shape: {X.shape}")
        print(f"âœ“ y shape: {y.shape}")
        
        # Train/Val/Test ë¶„í• 
        n_samples = len(X)
        test_idx = int(n_samples * (1 - self.test_size))
        val_idx = int(test_idx * (1 - self.val_size))
        
        self.X_train = X[:val_idx]
        self.X_val = X[val_idx:test_idx]
        self.X_test = X[test_idx:]
        
        self.y_train = y[:val_idx]
        self.y_val = y[val_idx:test_idx]
        self.y_test = y[test_idx:]
        
        print(f"âœ“ Train: {len(self.X_train)} samples")
        print(f"âœ“ Val:   {len(self.X_val)} samples")
        print(f"âœ“ Test:  {len(self.X_test)} samples")

    def build_model(self,
                    lstm_units=[64, 32],
                    dense_units=[16, 8],
                    dropout_rate=0.3,
                    l2_reg=1e-4,
                    learning_rate=0.001):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        print(f"\nğŸ—ï¸  ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        print(f"  LSTM: {lstm_units}")
        print(f"  Dense: {dense_units}")
        print(f"  Dropout: {dropout_rate}")
        
        n_features = len(self.feature_names)
        
        self.model = tf.keras.Sequential([
            layers.Input(shape=(self.seq_len, n_features)),
            
            # LSTM ë ˆì´ì–´
            layers.LSTM(lstm_units[0], return_sequences=True, 
                       dropout=dropout_rate, recurrent_dropout=dropout_rate/2,
                       kernel_regularizer=regularizers.l2(l2_reg)),
            layers.BatchNormalization(),
            
            layers.LSTM(lstm_units[1], return_sequences=False,
                       dropout=dropout_rate, recurrent_dropout=dropout_rate/2,
                       kernel_regularizer=regularizers.l2(l2_reg)),
            layers.BatchNormalization(),
            
            # Dense ë ˆì´ì–´
            layers.Dense(dense_units[0], activation='relu',
                        kernel_regularizer=regularizers.l2(l2_reg)),
            layers.Dropout(dropout_rate),
            layers.BatchNormalization(),
            
            layers.Dense(dense_units[1], activation='relu',
                        kernel_regularizer=regularizers.l2(l2_reg)),
            layers.Dropout(dropout_rate/2),
            
            # ì¶œë ¥
            layers.Dense(1)
        ])
        
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='huber',
            metrics=['mae']
        )
        
        print("âœ“ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")

    def train_model(self, epochs=100, batch_size=32, patience=20, verbose=1):
        """ëª¨ë¸ í›ˆë ¨"""
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (epochs={epochs}, batch_size={batch_size})")
        
        callbacks_list = [
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=patience,
                restore_best_weights=True,
                verbose=verbose
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                patience=patience//2,
                factor=0.5,
                min_lr=1e-7,
                verbose=verbose
            ),
            callbacks.ModelCheckpoint(
                'best_mcp_lstm_model.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=0
            )
        ]
        
        self.history = self.model.fit(
            self.X_train, self.y_train,
            validation_data=(self.X_val, self.y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks_list,
            verbose=verbose
        )
        
        print("\nâœ“ í•™ìŠµ ì™„ë£Œ!")
        return self.history

    def evaluate_model(self):
        """ëª¨ë¸ í‰ê°€"""
        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        results = {}
        
        for name, X, y_true in [
            ('train', self.X_train, self.y_train),
            ('val', self.X_val, self.y_val),
            ('test', self.X_test, self.y_test)
        ]:
            # ì˜ˆì¸¡
            y_pred_scaled = self.model.predict(X, verbose=0).flatten()
            
            # ì—­ë³€í™˜
            y_pred = self.target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true_orig = self.target_scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()
            
            # ë¡œê·¸ ë³€í™˜ ë˜ëŒë¦¬ê¸°
            if self.use_log_transform:
                y_pred = np.expm1(y_pred)
                y_true_orig = np.expm1(y_true_orig)
            
            # ìŒìˆ˜ í´ë¦¬í•‘
            y_pred = np.maximum(y_pred, 0)
            
            # í‰ê°€ ì§€í‘œ
            mae = mean_absolute_error(y_true_orig, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred))
            r2 = r2_score(y_true_orig, y_pred)
            
            # MAPE
            epsilon = 1e-10
            mape = np.mean(np.abs((y_true_orig - y_pred) / (y_true_orig + epsilon))) * 100
            
            results[name] = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'mape': mape,
                'y_true': y_true_orig,
                'y_pred': y_pred
            }
            
            print(f"\n  {name.upper():>5}: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.4f}, MAPE={mape:.1f}%")
        
        return results

    def save_model_and_scaler(self):
        """ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        # ëª¨ë¸ ì €ì¥
        os.makedirs('models', exist_ok=True)
        model_path = 'models/best_mcp_lstm_model.h5'
        self.model.save(model_path)
        print(f"  âœ“ ëª¨ë¸: {model_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'scaler': self.feature_scaler,
            'target_scaler': self.target_scaler,
            'sequence_length': self.seq_len,
            'target_col': self.target_col,
            'feature_names': self.feature_names,
            'n_features': len(self.feature_names),
            'use_log_transform': self.use_log_transform
        }
        
        metadata_path = 'models/mcp_model_metadata.pkl'
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
        print(f"  âœ“ ë©”íƒ€ë°ì´í„°: {metadata_path}")

# ===================================================================
# ë©”ì¸ ì‹¤í–‰
# ===================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # 1. Predictor ì´ˆê¸°í™”
        print("\nâš™ï¸  Predictor ì´ˆê¸°í™”")
        predictor = CompleteMCPPredictor(
            sequence_length=24,
            target_col='total_events',
            test_size=0.2,
            val_size=0.1,
            use_log_transform=True,
            handle_outliers=True
        )

        # 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        predictor.load_and_prepare_data(csv_path='data/lstm_ready_cluster_data.csv')

        # 3. ì‹œí€€ìŠ¤ ìƒì„±
        print("\nğŸ”„ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±")
        predictor.create_sequences()

        # 4. ëª¨ë¸ êµ¬ì¶•
        print("\nğŸ—ï¸  LSTM ëª¨ë¸ êµ¬ì¶•")
        predictor.build_model(
            lstm_units=[64, 32],
            dense_units=[16, 8],
            dropout_rate=0.3,
            l2_reg=1e-4,
            learning_rate=0.001
        )

        # 5. ëª¨ë¸ í›ˆë ¨
        print("\nğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        history = predictor.train_model(
            epochs=100,
            batch_size=32,
            patience=20,
            verbose=1
        )

        # 6. ëª¨ë¸ í‰ê°€
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€")
        results = predictor.evaluate_model()

        # 7. ëª¨ë¸ ì €ì¥
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥")
        predictor.save_model_and_scaler()

        # 8. ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 80)
        print("âœ… í›ˆë ¨ ì™„ë£Œ! ê²°ê³¼ ìš”ì•½:")
        print("=" * 80)

        for dataset, metrics in results.items():
            print(f"{dataset.upper():>12}: RÂ²={metrics['r2']:.4f}, "
                  f"MAE={metrics['mae']:.2f}, MAPE={metrics['mape']:.1f}%")

        # RÂ² í‰ê°€
        test_r2 = results.get('test', {}).get('r2', -999)
        if test_r2 > 0.7:
            print("\nğŸ‰ ìš°ìˆ˜í•œ ì„±ëŠ¥! (RÂ² > 0.7)")
        elif test_r2 > 0.5:
            print("\nâœ… ì–‘í˜¸í•œ ì„±ëŠ¥ (RÂ² > 0.5)")
        elif test_r2 > 0:
            print("\nâš ï¸  RÂ² ì–‘ìˆ˜ ë‹¬ì„±, ì¶”ê°€ íŠœë‹ ê¶Œì¥")
        else:
            print("\nâŒ RÂ² ìŒìˆ˜. ëª¨ë¸ ì¬ê²€í†  í•„ìš”")

        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - models/best_mcp_lstm_model.h5: í›ˆë ¨ëœ ëª¨ë¸")
        print("  - models/mcp_model_metadata.pkl: ë©”íƒ€ë°ì´í„°")

        return predictor, results

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

# ì‹¤í–‰
if __name__ == "__main__":
    predictor, results = main()

    if predictor is not None:
        print("\n" + "=" * 80)
        print("ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 80)
        print("\në‹¤ìŒ ë‹¨ê³„: ì´ì œ í•™ìŠµëœ ëª¨ë¸ì„ FastAPIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nì‹¤í–‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
